{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ã€€def\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import ast\n",
    "def compute_information_centrality_by_Laplacian(G: nx.Graph, node):\n",
    "    L = nx.laplacian_matrix(G).toarray()\n",
    "    n = G.number_of_nodes()\n",
    "    L_dagger = np.linalg.pinv(L)\n",
    "    effective_resistance_of_node = n * L_dagger[node][node] + np.trace(L_dagger)\n",
    "    information_centrality = n / effective_resistance_of_node\n",
    "    return information_centrality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example for karate\n",
    "input_file = \"../../data/karate.mtx\"\n",
    "with open(input_file, 'r') as file:\n",
    "    edges = [(int(a) - 1, int(b) - 1) for a, b in (line.split() for line in file)]\n",
    "G = nx.Graph()\n",
    "G.add_edges_from(edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read result\n",
    "target_nodes = []\n",
    "edge_need_remove = []\n",
    "result_file = \"../../result/karate_result.txt\"\n",
    "\n",
    "with open(result_file, \"r\") as file:\n",
    "    lines = file.readlines()\n",
    "    for line in lines:\n",
    "        if line.startswith(\"Target nodes:\"):\n",
    "            target_nodes = ast.literal_eval(line.split(\":\", 1)[1].strip())\n",
    "        elif line.startswith(\"Edges to remove per target node:\"):\n",
    "            edge_lines = lines[lines.index(line) + 2:]\n",
    "            edge_str = ''.join([l.strip() for l in edge_lines])\n",
    "            edge_str = edge_str.rstrip(',')\n",
    "            edge_str = edge_str.rstrip(']') + ']'\n",
    "            edges = ast.literal_eval(edge_str.strip())\n",
    "            edge_need_remove.extend(edges)\n",
    "# print(\"Target nodes:\", target_nodes)\n",
    "# print(\"Edges to remove per target node:\", edge_need_remove)\n",
    "pair_target_edges = []\n",
    "for idx, target in enumerate(target_nodes):\n",
    "    pair_target_edges.append([target, edge_need_remove[idx]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Information centrality of targets:  1.423238131167291\n",
      "Information centrality of targets:  0.8222515330010772\n"
     ]
    }
   ],
   "source": [
    "# Calculate score now\n",
    "sum_of_information_centrality = 0\n",
    "sum_of_information_centrality_origin = 0\n",
    "for target, edge_list in pair_target_edges:\n",
    "    A = G.copy()\n",
    "    A.remove_edges_from(edge_list)\n",
    "    information_centrality = compute_information_centrality_by_Laplacian(A, target)\n",
    "    information_centrality_origin = compute_information_centrality_by_Laplacian(G, target)\n",
    "    sum_of_information_centrality += information_centrality\n",
    "    sum_of_information_centrality_origin += information_centrality_origin\n",
    "average = sum_of_information_centrality / len(pair_target_edges)\n",
    "average_origin = sum_of_information_centrality_origin / len(pair_target_edges)\n",
    "print(\"Information centrality of targets: \", average_origin)\n",
    "print(\"Information centrality of targets: \", average)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "graph",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
